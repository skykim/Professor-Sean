import os
import uuid
import pytesseract
from PIL import Image
from dotenv import load_dotenv
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.schema import Document
import re

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì„¸ì…˜ ID ìƒì„±
session_id = uuid.uuid4()

def load_vectorstore(persist_directory: str = "./chroma_db") -> Chroma:
    """
    ì €ì¥ëœ Chroma ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        persist_directory (str): ë²¡í„°ìŠ¤í† ì–´ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        Chroma: ë¡œë“œëœ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    """
    embedding_function = UpstageEmbeddings(
        model="solar-embedding-1-large"
    )
    
    vectorstore = Chroma(
        persist_directory=persist_directory,
        embedding_function=embedding_function
    )
    
    return vectorstore


# RAG ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_rag_chain(vectorstore, api_key):
    retriever = vectorstore.as_retriever(k=5)
    print(retriever)
    from langchain_upstage import ChatUpstage
    chat = ChatUpstage(upstage_api_key=api_key)

    from langchain.chains import create_history_aware_retriever
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

    contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
    ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""
    
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    history_aware_retriever = create_history_aware_retriever(chat, retriever, contextualize_q_prompt)

    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain

    qa_system_prompt = """
                          ë‹¹ì‹ ì€ Seanêµìˆ˜ ì…ë‹ˆë‹¤. ì™¸í–¥ì ì´ê³  ì§ˆë¬¸ì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ì‹ì‚¬ í›„ ì¼€ì´í¬ë¥¼ ë¨¹ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤. ì²­ë°”ì§€ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤. ìˆ ì„ ê°€ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
                          ì§€ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”.
                          ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
                          ë‹µë³€ì€ ë‘ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.

    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    return rag_chain

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ë²¡í„°ìŠ¤í˜ì´ìŠ¤ ìƒì„±
vectorstore = load_vectorstore("./chroma_db")

# RAG ì²´ì¸ ìƒì„±
rag_chain = create_rag_chain(vectorstore, os.getenv("UPSTAGE_API_KEY"))

# ì§ˆë¬¸ ì…ë ¥ ë° ë‹µë³€ ì¶œë ¥
while True:
    question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
    if question.lower() == "exit":
        break
    
    # ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    result = rag_chain.invoke({"input": question, "chat_history": []})
    answer = result["answer"]
    context = result["context"]
    
    print("\nğŸ“ ë‹µë³€:", answer)
    print("ğŸ“ ì¦ê±°:", context)
    print("\n")